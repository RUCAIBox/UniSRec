n_layers: 2
n_heads: 2
hidden_size: 300
inner_size: 256
hidden_dropout_prob: 0.5
attn_dropout_prob: 0.5
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
item_attribute: 'class'
mask_ratio: 0.2
aap_weight: 1.0
mip_weight: 0.2
map_weight: 1.0
sp_weight: 0.5
train_stage: 'pretrain'
pretrain_epochs: 300
save_step: 100
pre_model_path: ''
loss_type: 'CE'

plm_suffix: feat1CLS
plm_size: 768
adapter_layers: [768, 300]
train_batch_size: 1024
